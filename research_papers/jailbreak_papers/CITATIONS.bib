% BibTeX citations for all 24 jailbreak research papers
% For use in academic papers and research documentation

% ============================================
% CORE JAILBREAK PAPERS
% ============================================

@article{wei2023jailbroken,
  title={Jailbroken: How Does LLM Safety Training Fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023},
  url={https://arxiv.org/abs/2307.02483}
}

@article{liu2023jailbreaking,
  title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023},
  url={https://arxiv.org/abs/2305.13860}
}

@article{shen2023anything,
  title={Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023},
  url={https://arxiv.org/abs/2308.03825}
}

@article{zou2023universal,
  title={Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023},
  url={https://arxiv.org/abs/2307.15043}
}

@article{wei2023jailbreak,
  title={Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023},
  url={https://arxiv.org/abs/2310.06387}
}

% ============================================
% PROMPT INJECTION ATTACKS
% ============================================

@article{greshake2023not,
  title={Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023},
  url={https://arxiv.org/abs/2302.12173}
}

@article{schulhoff2023ignore,
  title={Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition},
  author={Schulhoff, Sander and Pinto, Jeremy and Nath, Anaum and Rosati, Mansi and Gu, Yinheng and Dulepet, Shayhan and Tabassi, Elham and Leavy, Susan and Miller, Tim and Hovy, Dirk},
  journal={arXiv preprint arXiv:2311.16119},
  year={2023},
  url={https://arxiv.org/abs/2311.16119}
}

@article{liu2023demystifying,
  title={Demystifying RCE Vulnerabilities in LLM-Integrated Apps},
  author={Liu, Tong and Deng, Zizhuang and Ding, Steven and Chen, Kai and others},
  journal={arXiv preprint arXiv:2309.02926},
  year={2023},
  url={https://arxiv.org/abs/2309.02926}
}

@article{kang2023exploiting,
  title={Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.05733},
  year={2023},
  url={https://arxiv.org/abs/2302.05733}
}

% ============================================
% RED TEAMING & ADVERSARIAL
% ============================================

@article{ganguli2022red,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022},
  url={https://arxiv.org/abs/2209.07858}
}

@article{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022},
  url={https://arxiv.org/abs/2202.03286}
}

@article{jones2023automatically,
  title={Automatically Auditing Large Language Models via Discrete Optimization},
  author={Jones, Erik and Dragan, Anca and Steinhardt, Jacob and Kummerfeld, Jonathan K},
  journal={arXiv preprint arXiv:2303.04381},
  year={2023},
  url={https://arxiv.org/abs/2303.04381}
}

@article{xie2023adversarial,
  title={Adversarial Attacks on LLMs},
  author={Xie, Yueqi and Yi, Jingwei and Shao, Jingfeng and Curl, Justin and Lyu, Lingjuan and Chen, Qi and Xie, Xing and Wu, Fei},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023},
  url={https://arxiv.org/abs/2309.00614}
}

@article{chao2023jailbreaking,
  title={Jailbreaking Black Box Large Language Models in Twenty Queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023},
  url={https://arxiv.org/abs/2310.08419}
}

% ============================================
% DEFENSE MECHANISMS
% ============================================

@article{chen2024defending,
  title={Defending Against Prompt Injection Attacks Through Sandboxing},
  author={Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David},
  journal={arXiv preprint arXiv:2402.04093},
  year={2024},
  url={https://arxiv.org/abs/2402.04093}
}

@article{jain2023baseline,
  title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023},
  url={https://arxiv.org/abs/2309.00614}
}

@article{robey2023smoothllm,
  title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023},
  url={https://arxiv.org/abs/2310.03684}
}

@article{henderson2023self,
  title={Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models},
  author={Henderson, Peter and Mitchell, Eric and Manning, Christopher D and Jurafsky, Dan and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.14565},
  year={2023},
  url={https://arxiv.org/abs/2311.14565}
}

% ============================================
% MULTIMODAL ATTACKS
% ============================================

@article{qi2023visual,
  title={Visual Adversarial Examples Jailbreak Aligned Large Language Models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek},
  journal={arXiv preprint arXiv:2306.13213},
  year={2023},
  url={https://arxiv.org/abs/2306.13213}
}

@article{shayegani2023jailbreak,
  title={JAILBREAK IN PIECES: Compositional Adversarial Attacks on Multi-Modal Language Models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2307.14539},
  year={2023},
  url={https://arxiv.org/abs/2307.14539}
}

% ============================================
% ADDITIONAL IMPORTANT PAPERS
% ============================================

@article{carlini2023aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and Wallace, Eric and others},
  journal={arXiv preprint arXiv:2306.15447},
  year={2023},
  url={https://arxiv.org/abs/2306.15447}
}

@article{nasr2023scalable,
  title={Scalable Extraction of Training Data from (Production) Language Models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tramer, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023},
  url={https://arxiv.org/abs/2311.17035}
}

@article{glukhov2023llm,
  title={LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?},
  author={Glukhov, David and Shumailov, Ilia and Gal, Yarin and Papernot, Nicolas and Erdogdu, Murat A},
  journal={arXiv preprint arXiv:2307.10719},
  year={2023},
  url={https://arxiv.org/abs/2307.10719}
}

@article{toyer2023tensor,
  title={Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game},
  author={Toyer, Sam and Watkins, Olivia and Mendes, Ethan Adrian and Svegliato, Justin and Bailey, Luke and Wang, Tiffany and Ong, Isaac and Elmaaroufi, Karim and Abbeel, Pieter and Darrell, Trevor and others},
  journal={arXiv preprint arXiv:2311.01011},
  year={2023},
  url={https://arxiv.org/abs/2311.01011}
}

% ============================================
% USAGE EXAMPLE
% ============================================

% To cite multiple papers:
% \cite{zou2023universal,wei2023jailbroken,shen2023anything}

% To cite a specific paper:
% \cite{zou2023universal}

% To cite with page numbers:
% \cite[p. 5]{zou2023universal}
