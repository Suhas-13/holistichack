# üìä Jailbreak Papers - Summary Table

**Quick reference table for all 24 papers**

---

## Complete Paper List

| # | Title | Authors (Lead) | Institution | Date | Category | arXiv | Impact |
|---|-------|----------------|-------------|------|----------|-------|--------|
| 1 | Jailbroken: How Does LLM Safety Training Fail? | A. Wei | UC Berkeley | Jul 2023 | Theory | [2307.02483](https://arxiv.org/abs/2307.02483) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 2 | Jailbreaking ChatGPT via Prompt Engineering | Y. Liu | NTU | May 2023 | Empirical | [2305.13860](https://arxiv.org/abs/2305.13860) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 3 | Do Anything Now: In-The-Wild Jailbreak Prompts | X. Shen | CISPA | Aug 2023 | Empirical | [2308.03825](https://arxiv.org/abs/2308.03825) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 4 | Universal and Transferable Adversarial Attacks (GCG) | A. Zou | CMU | Jul 2023 | Attack | [2307.15043](https://arxiv.org/abs/2307.15043) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 5 | Jailbreak with Few In-Context Demonstrations | Z. Wei | Peking U | Oct 2023 | Attack | [2310.06387](https://arxiv.org/abs/2310.06387) | ‚≠ê‚≠ê‚≠ê |
| 6 | Indirect Prompt Injection | K. Greshake | CISPA | Feb 2023 | Injection | [2302.12173](https://arxiv.org/abs/2302.12173) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 7 | HackAPrompt: 600K+ Competition Attacks | S. Schulhoff | UMD | Nov 2023 | Dataset | [2311.16119](https://arxiv.org/abs/2311.16119) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 8 | RCE Vulnerabilities in LLM Apps | T. Liu | UIUC | Sep 2023 | Injection | [2309.02926](https://arxiv.org/abs/2309.02926) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 9 | Exploiting Programmatic Behavior | D. Kang | Stanford | Feb 2023 | Injection | [2302.05733](https://arxiv.org/abs/2302.05733) | ‚≠ê‚≠ê‚≠ê |
| 10 | Anthropic Red Teaming Methodology | D. Ganguli | Anthropic | Aug 2022 | Red Team | [2209.07858](https://arxiv.org/abs/2209.07858) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 11 | Red Teaming with Language Models | E. Perez | NYU/Anthropic | Feb 2022 | Red Team | [2202.03286](https://arxiv.org/abs/2202.03286) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 12 | ARCA: Automated Auditing | E. Jones | UC Berkeley | Mar 2023 | Red Team | [2303.04381](https://arxiv.org/abs/2303.04381) | ‚≠ê‚≠ê‚≠ê |
| 13 | Adversarial Attacks Survey | Y. Xie | Zhejiang U | Sep 2023 | Survey | [2309.00614](https://arxiv.org/abs/2309.00614) | ‚≠ê‚≠ê‚≠ê |
| 14 | PAIR: Jailbreaking in 20 Queries | P. Chao | UPenn | Oct 2023 | Attack | [2310.08419](https://arxiv.org/abs/2310.08419) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 15 | Sandboxing Defense | S. Chen | UC San Diego | Feb 2024 | Defense | [2402.04093](https://arxiv.org/abs/2402.04093) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 16 | Baseline Defenses | N. Jain | UMD | Sep 2023 | Defense | [2309.00614](https://arxiv.org/abs/2309.00614) | ‚≠ê‚≠ê‚≠ê |
| 17 | SmoothLLM Defense | A. Robey | UPenn | Oct 2023 | Defense | [2310.03684](https://arxiv.org/abs/2310.03684) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| 18 | Self-Destructing Models | P. Henderson | Stanford | Nov 2023 | Defense | [2311.14565](https://arxiv.org/abs/2311.14565) | ‚≠ê‚≠ê‚≠ê |
| 19 | Visual Adversarial Jailbreaks | X. Qi | Princeton | Jun 2023 | Multimodal | [2306.13213](https://arxiv.org/abs/2306.13213) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 20 | Jailbreak in Pieces | E. Shayegani | UC Riverside | Jul 2023 | Multimodal | [2307.14539](https://arxiv.org/abs/2307.14539) | ‚≠ê‚≠ê‚≠ê |
| 21 | Adversarially Aligned? | N. Carlini | DeepMind | Jun 2023 | Theory | [2306.15447](https://arxiv.org/abs/2306.15447) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 22 | Training Data Extraction | M. Nasr | DeepMind | Nov 2023 | Privacy | [2311.17035](https://arxiv.org/abs/2311.17035) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 23 | LLM Censorship Framing | D. Glukhov | Oxford | Jul 2023 | Theory | [2307.10719](https://arxiv.org/abs/2307.10719) | ‚≠ê‚≠ê‚≠ê |
| 24 | Tensor Trust: 126K+ Game Attacks | S. Toyer | UC Berkeley | Nov 2023 | Dataset | [2311.01011](https://arxiv.org/abs/2311.01011) | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## Legend

**Impact Rating**:
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê = Essential/Must-read (highly cited, foundational)
- ‚≠ê‚≠ê‚≠ê‚≠ê = Highly recommended (significant contribution)
- ‚≠ê‚≠ê‚≠ê = Recommended (specialized/niche)

**Categories**:
- **Theory**: Understanding why attacks/defenses work
- **Empirical**: Real-world data and observations
- **Attack**: Novel attack methodologies
- **Injection**: Prompt injection specific
- **Red Team**: Testing and evaluation methods
- **Defense**: Protection mechanisms
- **Survey**: Comprehensive overview
- **Multimodal**: Image/vision attacks
- **Dataset**: Large-scale data collection
- **Privacy**: Training data leakage

---

## Key Statistics by Category

| Category | Count | % of Total |
|----------|-------|------------|
| Attack Methods | 7 | 29% |
| Defense Methods | 4 | 17% |
| Red Teaming | 5 | 21% |
| Prompt Injection | 4 | 17% |
| Theory/Understanding | 3 | 13% |
| Multimodal | 2 | 8% |
| Datasets | 2 | 8% |
| Privacy | 1 | 4% |
| **TOTAL** | **24** | **100%** |

*Note: Some papers span multiple categories*

---

## Timeline Distribution

| Period | Paper Count | Key Developments |
|--------|-------------|------------------|
| 2022 | 2 | Early red teaming (Anthropic) |
| 2023 Q1 | 3 | Prompt injection emerges |
| 2023 Q2 | 5 | ChatGPT jailbreaking, visual attacks |
| 2023 Q3 | 7 | **Peak research** (GCG, DAN, PAIR) |
| 2023 Q4 | 6 | Defenses, large datasets |
| 2024+ | 1 | Advanced defenses |

---

## Institution Breakdown

| Institution | Papers | Notable Contributions |
|-------------|--------|----------------------|
| UC Berkeley | 3 | Theory, ARCA, Tensor Trust |
| Anthropic | 2 | Red teaming methodology |
| UPenn | 2 | PAIR, SmoothLLM |
| Google DeepMind | 2 | Alignment, privacy |
| CISPA | 2 | DAN prompts, indirect injection |
| Stanford | 2 | Programmatic, self-destructing |
| Princeton | 1 | Visual adversarial |
| CMU | 1 | GCG attack |
| UMD | 2 | HackAPrompt, baselines |
| Others | 7 | Various specializations |

---

## Papers with Released Code/Datasets

| Paper # | Resource | Link | Type |
|---------|----------|------|------|
| 4 | GCG Implementation | [GitHub](https://github.com/llm-attacks/llm-attacks) | Code |
| 7 | HackAPrompt Dataset | [GitHub](https://github.com/promptslab/hackprompt) | Dataset (600K+ attacks) |
| 24 | Tensor Trust Game | [Website](https://tensortrust.ai/) | Dataset (126K+ attacks) |
| 17 | SmoothLLM | GitHub | Code |

---

## Top 10 Most Cited (Estimated)

| Rank | Paper # | Title | Est. Citations |
|------|---------|-------|----------------|
| 1 | 4 | GCG Universal Attacks | ~500+ |
| 2 | 10 | Anthropic Red Teaming | ~300+ |
| 3 | 6 | Indirect Prompt Injection | ~200+ |
| 4 | 2 | Jailbreaking ChatGPT | ~150+ |
| 5 | 11 | Red Teaming with LLMs | ~100+ |
| 6 | 21 | Adversarially Aligned | ~80+ |
| 7 | 3 | Do Anything Now | ~70+ |
| 8 | 19 | Visual Adversarial | ~60+ |
| 9 | 14 | PAIR Attack | ~50+ |
| 10 | 17 | SmoothLLM | ~40+ |

*Note: Citation counts are estimates based on Google Scholar as of Nov 2024*

---

## Papers by Difficulty Level

### Beginner-Friendly
- **#2**: Jailbreaking ChatGPT (empirical, examples)
- **#3**: Do Anything Now (real prompts)
- **#7**: HackAPrompt (competition format)
- **#10**: Anthropic Red Teaming (methodology)

### Intermediate
- **#6**: Indirect Prompt Injection
- **#15**: Sandboxing Defense
- **#19**: Visual Adversarial
- **#14**: PAIR Attack

### Advanced
- **#1**: Safety Training Failure (theory)
- **#4**: GCG (optimization heavy)
- **#17**: SmoothLLM (provable guarantees)
- **#21**: Adversarially Aligned (deep theory)

---

## Reading Time Estimates

| Paper # | Pages | Est. Reading Time | Difficulty |
|---------|-------|-------------------|------------|
| 1 | 12 | 2-3 hours | Hard |
| 2 | 16 | 2-3 hours | Medium |
| 3 | 10 | 1-2 hours | Easy |
| 4 | 20 | 4-5 hours | Hard |
| 5 | 8 | 1-2 hours | Medium |
| 6 | 14 | 2-3 hours | Medium |
| 7 | 18 | 2-3 hours | Easy |
| 8 | 10 | 1-2 hours | Medium |
| 9 | 12 | 2 hours | Medium |
| 10 | 32 | 4-5 hours | Medium |
| 11 | 24 | 3-4 hours | Medium |
| 12 | 16 | 2-3 hours | Hard |
| 13 | 20 | 3 hours | Medium |
| 14 | 14 | 2-3 hours | Medium |
| 15 | 12 | 2 hours | Medium |
| 16 | 10 | 1-2 hours | Easy |
| 17 | 16 | 3-4 hours | Hard |
| 18 | 14 | 2-3 hours | Medium |
| 19 | 18 | 3 hours | Hard |
| 20 | 12 | 2 hours | Medium |
| 21 | 22 | 4 hours | Hard |
| 22 | 28 | 4-5 hours | Hard |
| 23 | 8 | 1-2 hours | Medium |
| 24 | 16 | 2-3 hours | Easy |

**Total Reading Time**: ~60-75 hours for all papers

---

## Quick Download Commands

### Download Top 5 Essential Papers
```bash
wget https://arxiv.org/pdf/2307.15043.pdf -O "gcg_attack.pdf"          # #4
wget https://arxiv.org/pdf/2305.13860.pdf -O "jailbreak_chatgpt.pdf"   # #2
wget https://arxiv.org/pdf/2209.07858.pdf -O "anthropic_redteam.pdf"   # #10
wget https://arxiv.org/pdf/2310.08419.pdf -O "pair_attack.pdf"         # #14
wget https://arxiv.org/pdf/2310.03684.pdf -O "smoothllm.pdf"           # #17
```

### Download by Category
```bash
# Attack Methods
wget https://arxiv.org/pdf/2307.15043.pdf https://arxiv.org/pdf/2310.08419.pdf https://arxiv.org/pdf/2310.06387.pdf

# Defenses
wget https://arxiv.org/pdf/2402.04093.pdf https://arxiv.org/pdf/2310.03684.pdf

# Prompt Injection
wget https://arxiv.org/pdf/2302.12173.pdf https://arxiv.org/pdf/2311.16119.pdf https://arxiv.org/pdf/2309.02926.pdf

# Multimodal
wget https://arxiv.org/pdf/2306.13213.pdf https://arxiv.org/pdf/2307.14539.pdf
```

---

## Papers Relevant to Specific Models

### Claude (Anthropic)
- **#10**: Anthropic Red Teaming ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **#11**: Red Teaming with LLMs
- **#4**: GCG (tested on Claude)
- **#1**: Safety Training Theory

### GPT/ChatGPT (OpenAI)
- **#2**: Jailbreaking ChatGPT ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **#3**: Do Anything Now
- **#7**: HackAPrompt (GPT-focused)

### Vision Models (GPT-4V, Claude 3.5 Sonnet, Gemini)
- **#19**: Visual Adversarial ‚≠ê‚≠ê‚≠ê‚≠ê
- **#20**: Jailbreak in Pieces
- **#4**: GCG (multimodal version)

---

## Papers by Attack Success Rate

| Paper # | Reported Success Rate | Target Models |
|---------|----------------------|---------------|
| 4 (GCG) | ~80-90% | GPT-4, Claude, Vicuna |
| 14 (PAIR) | ~60-80% (20 queries) | GPT-3.5, GPT-4 |
| 3 (DAN) | ~40-60% (varies) | ChatGPT |
| 5 (ICL) | ~50-70% | Various |
| 19 (Visual) | ~60-80% | GPT-4V, LLaVA |

*Note: Success rates vary by defense version and test conditions*

---

**Last Updated**: November 2024
**For Full Details**: See README.md
**For Citations**: See CITATIONS.bib
**For Quick Access**: See QUICK_REFERENCE.md
